for the tokenHelper 
there are 1.access token -> memory/localStorage
          2.Refresh token -> httpOnly cookie save from XSS
           these tokens are used for identify and authorize the user

After user succefully logged in then this tokenHelper function create a token,refreshtoken
that willbe used for backend authenticate the future request

to get the token we need jwt -> we get it from the github.com/dgrijalva/jwt-go
        this jwt has this jwt.Strandclaims that will give us the expiry and issur etc..




for the signup function in usercontroller we need the ctx,cancel as they used for MONGODB operation need the context(ctx)
After the 100 seconds auto cancel -> revent the request hanging forever


"


subscriptionModels intution:
Q1: Separate collection for many-to-many relationship, performance, and flexibility.

Q2: Compound unique index on (user_id, source_id) prevents duplicate subscriptions.

Q3: user_id is string to match JWT token format and existing User model.

Q4: Delete subscription on unsubscribe (simpler). Keep source data (other users may use it).

Q5: 
Step 1: Find subscriptions where user_id = X
Step 2: Extract all source_id values
Step 3: Query sources collection with those IDs




========================================
ARTICLE MODEL - COMPLETE UNDERSTANDING
========================================

Q1: Why do we need both content_hash AND url for deduplication?
-------------------------------------------------------------------
Answer:
- content_hash = Detects SAME CONTENT at different URLs (syndication)
- url = Detects SAME URL with different content (updates)

Example:
  Article A: url="site1.com/post", content_hash="abc123"
  Article B: url="site2.com/news", content_hash="abc123"  → Duplicate content!
  Article C: url="site1.com/post", content_hash="xyz789"  → Content updated!

Strategy:
- Use (source_id, url) unique index → prevent re-crawling same URL per source
- Use content_hash index → detect syndicated content across sources


Q2: What fields should be pointers (*string, *time.Time) vs values?
---------------------------------------------------------------------
Answer:

POINTERS (*) = OPTIONAL fields (can be nil/null):
  ✓ Summary *string        → Not all articles have summaries
  ✓ Published_at *time.Time → Some sites don't show publish date
  ✓ Author *string         → Not all articles show author

VALUES = REQUIRED fields (always have data):
  ✓ Title string           → Every article must have title
  ✓ URL string             → Every article must have URL
  ✓ Content_hash string    → Crawler always computes this
  ✓ Discovered_at time.Time → Crawler always sets when found
  ✓ Source_id ObjectID     → Article must belong to a source

Rule: If field might not exist → use pointer. If always exists → use value.


Q3: What indexes do we need on the articles collection?
---------------------------------------------------------
Answer:

1. COMPOUND UNIQUE INDEX: (source_id, url)
   Purpose: Prevent duplicate URLs per source
   Query: "Does this URL already exist for this source?"

2. SIMPLE INDEX: source_id
   Purpose: Find all articles from a source
   Query: "Get all articles from source X"

3. SIMPLE INDEX: published_at (DESC)
   Purpose: Sort articles newest first
   Query: "Get latest 20 articles"

4. SIMPLE INDEX: content_hash
   Purpose: Fast duplicate detection
   Query: "Does this content already exist anywhere?"

5. SIMPLE INDEX: discovered_at (DESC)
   Purpose: Track recent discoveries
   Query: "What articles were found in last 24 hours?"

MongoDB commands:
  db.articles.createIndex({ source_id: 1, url: 1 }, { unique: true })
  db.articles.createIndex({ source_id: 1 })
  db.articles.createIndex({ published_at: -1 })
  db.articles.createIndex({ content_hash: 1 })
  db.articles.createIndex({ discovered_at: -1 })


Q4: Why track both published_at AND discovered_at?
----------------------------------------------------
Answer:

published_at = When article was ORIGINALLY published (from website)
discovered_at = When OUR CRAWLER found it (always set by us)

Example 1 - Recent news:
  Website says: "Published: Jan 10, 2025, 9:00 AM"
  Crawler finds it: Jan 10, 2025, 10:30 AM
  → published_at = Jan 10, 9:00 AM
  → discovered_at = Jan 10, 10:30 AM

Example 2 - Old blog post:
  Website says: "Published: Dec 1, 2020"
  Crawler finds it: Jan 15, 2025 (first time crawling this site)
  → published_at = Dec 1, 2020
  → discovered_at = Jan 15, 2025

Example 3 - No publish date:
  Website doesn't show publish date
  Crawler finds it: Jan 15, 2025
  → published_at = nil (null)
  → discovered_at = Jan 15, 2025

Why both?
  ✓ Feed sorts by published_at (users want newest content)
  ✓ If published_at is missing, fallback to discovered_at
  ✓ discovered_at helps track crawler performance


Q5: How will you enforce "keep only 50 latest articles per source"?
---------------------------------------------------------------------
Answer:

Strategy: After inserting new article, delete oldest ones if count > 50

Step 1: Count articles for this source
  count = db.articles.count({ source_id: "123" })

Step 2: If count > 50, delete oldest
  Find articles sorted by published_at DESC, skip first 50
  Delete the rest

MongoDB query (conceptual):
  // Find oldest articles (those beyond the 50 newest)
  oldArticles = db.articles.find({ source_id: "123" })
                           .sort({ published_at: -1 })
                           .skip(50)
  
  // Delete them
  db.articles.deleteMany({ _id: { $in: oldArticles.map(a => a._id) } })

This is called "sliding window" - always keep 50 newest, discard older.


BONUS: Feed Query Example
---------------------------
Query: "Get 20 most recent articles from sources [A, B, C]"

MongoDB:
  db.articles.find({ 
    source_id: { $in: [ObjectId("A"), ObjectId("B"), ObjectId("C")] }
  })
  .sort({ published_at: -1 })
  .limit(20)

Go (concept):
  sourceIDs := []primitive.ObjectID{idA, idB, idC}
  cursor, _ := articleCollection.Find(
      ctx,
      bson.M{"source_id": bson.M{"$in": sourceIDs}},
      options.Find().SetSort(bson.D{{Key: "published_at", Value: -1}}).SetLimit(20),
  )


========================================
TASK 4: DATABASE INDEXES - UNDERSTANDING
========================================

Q1: Why create a compound index on (user_id, source_id) for subscriptions instead of two separate indexes?
Answer:
- Compound index (user_id, source_id) serves TWO purposes:
  1. UNIQUENESS: Prevents same user from subscribing to same source twice
  2. FAST LOOKUP: MongoDB can quickly find "Is user X subscribed to source Y?"

- Two separate indexes would:
  ✓ Allow fast lookup by user_id
  ✓ Allow fast lookup by source_id
  ✗ But NOT enforce uniqueness on the combination
  ✗ Cannot prevent duplicate (user_id, source_id) pairs

Example:
  Without compound index: User "ABC" could subscribe to Source "123" multiple times
  With compound index: MongoDB rejects duplicate (ABC, 123) pairs


Q2: Why does the articles collection need index on published_at with value -1 (descending)?
Answer:
- Feed queries need articles sorted NEWEST FIRST (reverse chronological order)
- Value -1 = descending order (newest → oldest)
- Value 1 = ascending order (oldest → newest)

Query example:
  db.articles.find().sort({ published_at: -1 }).limit(20)
  → Returns 20 most recent articles

Without index: MongoDB scans ALL articles then sorts (SLOW)
With index -1: MongoDB reads articles in pre-sorted order (FAST)


Q3: What happens if you try to insert a duplicate URL in sources collection after creating unique index?
Answer:
MongoDB will REJECT the insert operation with an error:

Error: "E11000 duplicate key error collection: go-auth.sources index: url_unique"

This protects data integrity - prevents same publisher URL from being added twice.

In Go code, you'll catch this error:
  if mongo.IsDuplicateKeyError(err) {
      return "URL already exists"
  }


Q4: Why do we need BOTH indexes: (source_id, url) compound AND source_id simple for articles?
Answer:
They serve DIFFERENT purposes:

1. Compound index (source_id, url):
   Purpose: UNIQUENESS - prevent duplicate URLs within same source
   Query: "Does this URL already exist for this source?"
   Example: source "ABC" can't have two articles with URL "example.com/post1"

2. Simple index (source_id):
   Purpose: PERFORMANCE - fast retrieval of ALL articles from a source
   Query: "Get all articles from source ABC"
   Example: db.articles.find({ source_id: "ABC" })

MongoDB optimization rule:
- Compound index (source_id, url) can help queries on source_id alone
- BUT it's not as efficient as dedicated simple index
- For frequently accessed fields, create both


Q5: When should EnsureIndexes() be called? In every API request or once at startup?
Answer:
ONCE AT STARTUP only.

Reasons:
✓ Index creation is EXPENSIVE (takes time, locks collection temporarily)
✓ Indexes persist in MongoDB (don't need recreation)
✓ Creating on every request would slow down ALL requests massively
✓ Indexes only need recreation if dropped or schema changes

Best practice:
- Call EnsureIndexes() in main.go after DB connection
- Before starting HTTP server
- If index already exists, MongoDB ignores CreateOne() (safe to call multiple times)

Wrong approach:
  ✗ Calling in API handlers
  ✗ Calling before every query
  ✗ Calling in middleware


========================================
KEY TAKEAWAYS
========================================

1. Indexes = Speed + Data Integrity
   - Without indexes: queries scan entire collection (slow)
   - With indexes: direct lookup (fast)
   - Unique indexes: prevent duplicate data

2. Compound indexes enforce uniqueness on COMBINATIONS
   - (user_id, source_id) unique = user can't subscribe twice to same source
   - (source_id, url) unique = source can't have duplicate URLs

3. Index direction matters for sorting
   - Value 1 = ascending (A→Z, old→new)
   - Value -1 = descending (Z→A, new→old)
   - Match your query sort order for best performance

4. Create indexes at startup, not per-request
   - Index creation is expensive
   - Indexes persist in database
   - Safe to call CreateOne() multiple times (idempotent)

5. Index naming helps debugging
   - SetName("user_source_unique") makes it easy to identify
   - View indexes in MongoDB Compass or shell: db.collection.getIndexes()

